# DDP experiments for the Adversarial Autoencoder (AAE) model.

Our scaling study measures AAE training performance for the following scanarios:

| Nodes       | GPUs        | Global Batch Size |
|    :----:   |    :----:   |    :----:   |
| 1           | 4           | 128         |
| 2           | 8           | 256         |
| 4           | 16          | 512         |
| 8           | 32          | 1024        |

Note: Each GPU has 16GB of memory which can fit a maximum local batch size of 32.

The aae_template.yaml file stores the model hyperparameters and training settings.

We have prepared sbatch submission scripts for each of the above experiments. They can be run as follows:
```
chmod +x ./ddp_aae_experiments/bin/submit_1-node_128-gbs.sh
chmod +x ./ddp_aae_experiments/bin/submit_2-node_256-gbs.sh
chmod +x ./ddp_aae_experiments/bin/submit_4-node_512-gbs.sh
chmod +x ./ddp_aae_experiments/bin/submit_8-node_1024-gbs.sh

sbatch ./ddp_aae_experiments/bin/submit_1-node_128-gbs.sh
sbatch ./ddp_aae_experiments/bin/submit_2-node_256-gbs.sh
sbatch ./ddp_aae_experiments/bin/submit_4-node_512-gbs.sh
sbatch ./ddp_aae_experiments/bin/submit_8-node_1024-gbs.sh
```

***

We report training benchmark results in the following table:

| Nodes       | GPUs        | Global Batch Size | Samples/second | Seconds/batch | Seconds/epoch |
|    :----:   |    :----:   |    :----:   |    :----:   |    :----:   |    :----:   |
| 1           | 4           | 128         | 977         | 0.131       | 107.158     |
| 2           | 8           | 256         | 1954        | 0.131       | 53.579      |
| 4           | 16          | 512         | 3909        | 0.131       | 26.780      |
| 8           | 32          | 1024        | 7816        | 0.131       | 13.395      |

Note: The code reports Seconds/batch and we use this to compute Seconds/epoch and Samples/second. Since we reserve 20% of the data for validation, we are left with 104704 training examples. We compute the following:
```
Seconds/epoch = (104704 / GlobalBatchSize) * .131 Seconds/batch
Samples/second = 104704 / (Seconds/epoch)
```
***

Here are the loss curves for the above runs, generated by running the following commands:
```
idev -m 5 -n 1 -N 1
module load conda
conda activate /scratch/06079/tg853783/ddmd/envs/pytorch.mpi
cd /scratch/06079/tg853783/ddmd/src/DeepDriveMD-Longhorn-2021/ddp_aae_experiments
python generate_loss_curves.py
```

![DiscriminatorTrainingLoss](img/DiscriminatorTrainingLoss.png)
![ReconstructionTrainingLoss](img/ReconstructionTrainingLoss.png)
![ValidationLoss](img/ValidationLoss.png)
